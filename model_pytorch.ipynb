{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorm0202/temas_selectos_CD-23/blob/main/model_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJ9AWKQ9OvT"
      },
      "source": [
        "# Modelos de predicciÃ³n con Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "rujfTEZm9gTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac88dba8-7dd0-447c-b0f3-7ab285f012a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4147 sha256=3f9deadf84b4e4ac88dbcc40594a491f7602f5d611c6c8627b8eb430fa02066b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDijnEe69OvY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchviz import make_dot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AgZeDM99OvY"
      },
      "source": [
        "# A Simple Regression Problem\n",
        "\n",
        "$$\n",
        "\\Large y = b + w x + \\epsilon\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE-R0uG69OvY"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkmpFEB69OvZ"
      },
      "source": [
        "### Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJbm80Gi9OvZ"
      },
      "outputs": [],
      "source": [
        "true_b = 1\n",
        "true_w = 2\n",
        "N = 100\n",
        "\n",
        "# Data Generation\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(N, 1)\n",
        "epsilon = (.1 * np.random.randn(N, 1))\n",
        "y = true_b + true_w * x + epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqIS2Zd59Ova"
      },
      "outputs": [],
      "source": [
        "# Shuffles the indices\n",
        "idx = np.arange(N)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# Uses first 80 random indices for train\n",
        "train_idx = idx[:int(N*.8)]\n",
        "# Uses the remaining indices for validation\n",
        "val_idx = idx[int(N*.8):]\n",
        "\n",
        "# Generates train and validation sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYh-r_nq9Ovb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "56b394fd-3728-41df-8bed-13309df60121"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "ax[0].scatter(x_train, y_train)\n",
        "ax[0].set_xlabel('x')\n",
        "ax[0].set_ylabel('y')\n",
        "ax[0].set_ylim([0, 3.1])\n",
        "ax[0].set_title('Generated Data - Train')\n",
        "\n",
        "ax[1].scatter(x_val, y_val, c='r')\n",
        "ax[1].set_xlabel('x')\n",
        "ax[1].set_ylabel('y')\n",
        "ax[1].set_ylim([0, 3.1])\n",
        "ax[1].set_title('Generated Data - Validation')\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkR2OPKN9Ovb"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2D5za2V9Ovb"
      },
      "source": [
        "## Step 0: Random Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAZ_UBm99Ovc"
      },
      "outputs": [],
      "source": [
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "np.random.seed(42)\n",
        "b = np.random.randn(1)\n",
        "w = np.random.randn(1)\n",
        "\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3ou8Jmh9Ovc"
      },
      "source": [
        "## Step 1: Compute Model's Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4QTWZmR9Ovc"
      },
      "outputs": [],
      "source": [
        "# Step 1 - Computes our model's predicted output - forward pass\n",
        "yhat = b + w * x_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfc8UhC09Ovc"
      },
      "source": [
        "## Step 2: Compute the Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fGLSZVN9Ovd"
      },
      "outputs": [],
      "source": [
        "# Step 2 - Computing the loss\n",
        "# We are using ALL data points, so this is BATCH gradient\n",
        "# descent. How wrong is our model? That's the error!\n",
        "error = (yhat - y_train)\n",
        "\n",
        "# It is a regression, so it computes mean squared error (MSE)\n",
        "loss = (error ** 2).mean()\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvHVB6749Ovd"
      },
      "source": [
        "## Step 3: Compute the Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1QIrONu9Ovd"
      },
      "outputs": [],
      "source": [
        "# Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "b_grad = 2 * error.mean()\n",
        "w_grad = 2 * (x_train * error).mean()\n",
        "print(b_grad, w_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ6ZEOtO9Ovd"
      },
      "source": [
        "## Step 4: Update the Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cRaQrOD9Ove"
      },
      "outputs": [],
      "source": [
        "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
        "lr = 0.1\n",
        "print(b, w)\n",
        "\n",
        "# Step 4 - Updates parameters using gradients and\n",
        "# the learning rate\n",
        "b = b - lr * b_grad\n",
        "w = w - lr * w_grad\n",
        "\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WWxh_7N9Ove"
      },
      "source": [
        "## Step 5: Rinse and Repeat!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INnZ-RPZ9Ove"
      },
      "outputs": [],
      "source": [
        "# Go back to Step 1 and run observe how your parameters b and w change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5qC4ARU9Ove"
      },
      "source": [
        "# Linear Regression in Numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK1eJgYz9Ovf"
      },
      "outputs": [],
      "source": [
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "np.random.seed(42)\n",
        "b = np.random.randn(1)\n",
        "w = np.random.randn(1)\n",
        "\n",
        "print(b, w)\n",
        "\n",
        "# Sets learning rate - this is \"eta\" ~ the \"n\"-like Greek letter\n",
        "lr = 0.1\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Step 1 - Computes model's predicted output - forward pass\n",
        "    yhat = b + w * x_train\n",
        "\n",
        "    # Step 2 - Computes the loss\n",
        "    # We are using ALL data points, so this is BATCH gradient\n",
        "    # descent. How wrong is our model? That's the error!\n",
        "    error = (yhat - y_train)\n",
        "    # It is a regression, so it computes mean squared error (MSE)\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "    b_grad = 2 * error.mean()\n",
        "    w_grad = 2 * (x_train * error).mean()\n",
        "\n",
        "    # Step 4 - Updates parameters using gradients and\n",
        "    # the learning rate\n",
        "    b = b - lr * b_grad\n",
        "    w = w - lr * w_grad\n",
        "\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b[0]"
      ],
      "metadata": {
        "id": "TF_SHpy6AbSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3IQoCOb9Ovf"
      },
      "outputs": [],
      "source": [
        "# Sanity Check: do we get the same results as our\n",
        "# gradient descent?\n",
        "linr = LinearRegression()\n",
        "linr.fit(x_train, y_train)\n",
        "print(linr.intercept_, linr.coef_[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9_Ecqre9Ovf"
      },
      "outputs": [],
      "source": [
        "b_minimum, w_minimum = b[0], w[0]\n",
        "# Generates evenly spaced x feature\n",
        "x_range = np.linspace(0, 1, 101)\n",
        "# Computes yhat\n",
        "yhat_range = b_minimum + w_minimum * x_range\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_ylim([0, 3.1])\n",
        "\n",
        "# Dataset\n",
        "ax.scatter(x_train, y_train)\n",
        "# Predictions\n",
        "ax.plot(x_range, yhat_range, label='Final model\\'s predictions', c='k', linestyle='--')\n",
        "\n",
        "# Annotations\n",
        "ax.annotate('b = {:.4f} w = {:.4f}'.format(b_minimum, w_minimum), xy=(.4, 1.5), c='k', rotation=34)\n",
        "ax.legend(loc=0)\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6FH15Sm9Ovg"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD0eGdQe9Ovh"
      },
      "source": [
        "## Loading Data, Devices and CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3CmopUE9Ovi"
      },
      "outputs": [],
      "source": [
        "x_train_tensor = torch.as_tensor(x_train)\n",
        "x_train.dtype, x_train_tensor.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1FrtuMp9Ovi"
      },
      "source": [
        "### Defining your device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9q9GRx99Ovi"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDaY_9wJ9Ovi"
      },
      "outputs": [],
      "source": [
        "n_cudas = torch.cuda.device_count()\n",
        "for i in range(n_cudas):\n",
        "    print(torch.cuda.get_device_name(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cashSBTu9Ovj"
      },
      "outputs": [],
      "source": [
        "# Our data was in Numpy arrays, but we need to transform them\n",
        "# into PyTorch's Tensors and then we send them to the\n",
        "# chosen device\n",
        "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
        "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aNaq2iQ9Ovj"
      },
      "outputs": [],
      "source": [
        "# Here we can see the difference - notice that .type() is more\n",
        "# useful since it also tells us WHERE the tensor is (device)\n",
        "print(type(x_train), type(x_train_tensor), x_train_tensor.type())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back to numpy: **Numpy cannot handle GPU tensors**â¦ you need to make them CPU tensors first using cpu()"
      ],
      "metadata": {
        "id": "VV4i0OOXB9xh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBOYEmhP9Ovj"
      },
      "outputs": [],
      "source": [
        "back_to_numpy = x_train_tensor.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymlyWHP89Ovj"
      },
      "outputs": [],
      "source": [
        "back_to_numpy = x_train_tensor.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts8IY4Rl9Ovj"
      },
      "source": [
        "## Creating Parameters\n",
        "\n",
        "We will apply gradient descent, and we want to run it on a GPU, but, the order is important. Observe the next two cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aOOQVh89Ovj"
      },
      "outputs": [],
      "source": [
        "# Initializes parameters \"b\" and \"w\" randomly. Since we want to apply gradient descent on\n",
        "# these parameters we need to set REQUIRES_GRAD = TRUE\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkcjc1RC9Ovk"
      },
      "outputs": [],
      "source": [
        "# now, we send them to device\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float).to(device)\n",
        "print(b, w)\n",
        "# Sorry, but NO! The to(device) \"shadows\" the gradient..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two options to do that correctly:\n",
        "- We can either create regular tensors and send them to the device (as we did with our data), and THEN set them as requiring gradients...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u37tEP0QEthM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeooHHIF9Ovk"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, dtype=torch.float).to(device)\n",
        "w = torch.randn(1, dtype=torch.float).to(device)\n",
        "# and THEN set them as requiring gradients...\n",
        "b.requires_grad_()\n",
        "w.requires_grad_()\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-RIQp3y9Ovk"
      },
      "source": [
        "- Or, We can specify the device at the moment of creation (RECOMMENDED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go1140uj9Ovk"
      },
      "outputs": [],
      "source": [
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47AFJlW19Ovk"
      },
      "source": [
        "# Autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNZRY-EE9Ovm"
      },
      "outputs": [],
      "source": [
        "# learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Step 1 - Computes model's predicted output - forward pass\n",
        "    yhat = b + w * x_train_tensor\n",
        "\n",
        "    # Step 2 - Computes the loss (MSE)\n",
        "    # Observe that we are using ALL data points, so this is BATCH gradient descent\n",
        "    error = (yhat - y_train_tensor)\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Step 4 - Updates parameters using gradients and the learning rate\n",
        "    # IMPORTANT: We need to use NO_GRAD to keep the update OUT of the gradient computation. It sets REQUIRES_GRAD to FALSE temporaly\n",
        "    # Because we are updating the parameters, we DON'T NEED the DYNAMIC GRAPH that PyTorch uses...\n",
        "    with torch.no_grad():\n",
        "        b -= lr * b.grad\n",
        "        w -= lr * w.grad\n",
        "\n",
        "    # Clear computed gradients\n",
        "    b.grad.zero_()\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_minimum, w_minimum = b.cpu().detach().numpy()[0], w.cpu().detach().numpy()[0]\n",
        "# Generates evenly spaced x feature\n",
        "x_range = np.linspace(0, 1, 101)\n",
        "# Computes yhat\n",
        "yhat_range = b_minimum + w_minimum * x_range\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_ylim([0, 3.1])\n",
        "\n",
        "# Dataset\n",
        "ax.scatter(x_train, y_train)\n",
        "# Predictions\n",
        "ax.plot(x_range, yhat_range, label='Final model\\'s predictions', c='k', linestyle='--')\n",
        "\n",
        "# Annotations\n",
        "ax.annotate('b = {:.4f} w = {:.4f}'.format(b_minimum, w_minimum), xy=(.4, 1.5), c='k', rotation=34)\n",
        "ax.legend(loc=0)\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "DvnovpZOLzHV",
        "outputId": "cfd56d5e-541d-409b-e161-7f2bb21b5d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSnkqKPl9Ovm"
      },
      "source": [
        "# Dynamic Computation Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZqjq8bO9Ovm"
      },
      "outputs": [],
      "source": [
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "# Step 1 - Computes our model's predicted output - forward pass\n",
        "yhat = b + w * x_train_tensor\n",
        "\n",
        "# Step 2 - Computes the loss\n",
        "error = (y_train_tensor- yhat)\n",
        "loss = (error ** 2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **<font color='blue'>Blue boxes</font>** correspond to the tensors we use as parameters, the ones weâre asking PyTorch to compute gradients for\n",
        "- **<font color='gray'>Gray box</font>** a Python operation that involves a gradient-computing tensor or its dependencies\n",
        "- **<font color='green'>Green box</font>** the same as the gray box, except it is the starting point for the computation of gradients (assuming the `backward()`method is called from the variable used to visualize the graph)â they are computed from the bottom-up in a graph"
      ],
      "metadata": {
        "id": "anj8R8mZOBNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can try plotting the graph for any python variable: yhat, error, loss...\n",
        "make_dot(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "c-88TTPlM_yF",
        "outputId": "82684c8d-2751-4140-df64-c2c59b8ecae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f4099c71890>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"222pt\" height=\"448pt\"\n viewBox=\"0.00 0.00 222.00 448.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 444)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-444 218,-444 218,4 -4,4\"/>\n<!-- 139915436202992 -->\n<g id=\"node1\" class=\"node\">\n<title>139915436202992</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"133.5,-31 79.5,-31 79.5,0 133.5,0 133.5,-31\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 139915434597456 -->\n<g id=\"node2\" class=\"node\">\n<title>139915434597456</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"154,-86 59,-86 59,-67 154,-67 154,-86\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 139915434597456&#45;&gt;139915436202992 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139915434597456&#45;&gt;139915436202992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.5,-66.9688C106.5,-60.1289 106.5,-50.5621 106.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.0001,-41.3678 106.5,-31.3678 103.0001,-41.3678 110.0001,-41.3678\"/>\n</g>\n<!-- 139915434598352 -->\n<g id=\"node3\" class=\"node\">\n<title>139915434598352</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"151,-141 62,-141 62,-122 151,-122 151,-141\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 139915434598352&#45;&gt;139915434597456 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139915434598352&#45;&gt;139915434597456</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.5,-121.9197C106.5,-114.9083 106.5,-105.1442 106.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.0001,-96.3408 106.5,-86.3408 103.0001,-96.3409 110.0001,-96.3408\"/>\n</g>\n<!-- 139915434597584 -->\n<g id=\"node4\" class=\"node\">\n<title>139915434597584</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"151,-196 62,-196 62,-177 151,-177 151,-196\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 139915434597584&#45;&gt;139915434598352 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139915434597584&#45;&gt;139915434598352</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.5,-176.9197C106.5,-169.9083 106.5,-160.1442 106.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.0001,-151.3408 106.5,-141.3408 103.0001,-151.3409 110.0001,-151.3408\"/>\n</g>\n<!-- 139915434597264 -->\n<g id=\"node5\" class=\"node\">\n<title>139915434597264</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"151,-251 62,-251 62,-232 151,-232 151,-251\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 139915434597264&#45;&gt;139915434597584 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139915434597264&#45;&gt;139915434597584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M106.5,-231.9197C106.5,-224.9083 106.5,-215.1442 106.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.0001,-206.3408 106.5,-196.3408 103.0001,-206.3409 110.0001,-206.3408\"/>\n</g>\n<!-- 139915434596560 -->\n<g id=\"node6\" class=\"node\">\n<title>139915434596560</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-306 0,-306 0,-287 101,-287 101,-306\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139915434596560&#45;&gt;139915434597264 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139915434596560&#45;&gt;139915434597264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M60.2545,-286.9197C68.1865,-279.1293 79.5788,-267.9405 89.0712,-258.6176\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"91.7982,-260.845 96.4802,-251.3408 86.8932,-255.8509 91.7982,-260.845\"/>\n</g>\n<!-- 139915434625168 -->\n<g id=\"node7\" class=\"node\">\n<title>139915434625168</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-373 23.5,-373 23.5,-342 77.5,-342 77.5,-373\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139915434625168&#45;&gt;139915434596560 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139915434625168&#45;&gt;139915434596560</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-341.791C50.5,-334.0249 50.5,-324.5706 50.5,-316.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-316.0647 50.5,-306.0648 47.0001,-316.0648 54.0001,-316.0647\"/>\n</g>\n<!-- 139915435073616 -->\n<g id=\"node8\" class=\"node\">\n<title>139915435073616</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-306 119,-306 119,-287 208,-287 208,-306\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 139915435073616&#45;&gt;139915434597264 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139915435073616&#45;&gt;139915434597264</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M153.5714,-286.9197C145.4169,-279.0514 133.6697,-267.7164 123.9508,-258.3385\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"126.3252,-255.7659 116.6987,-251.3408 121.4646,-260.8032 126.3252,-255.7659\"/>\n</g>\n<!-- 139915434597840 -->\n<g id=\"node9\" class=\"node\">\n<title>139915434597840</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"214,-367 113,-367 113,-348 214,-348 214,-367\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-355\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139915434597840&#45;&gt;139915435073616 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139915434597840&#45;&gt;139915435073616</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M163.5,-347.9688C163.5,-339.5131 163.5,-326.8901 163.5,-316.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.0001,-316.1656 163.5,-306.1656 160.0001,-316.1657 167.0001,-316.1656\"/>\n</g>\n<!-- 139915435522384 -->\n<g id=\"node10\" class=\"node\">\n<title>139915435522384</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"190.5,-440 136.5,-440 136.5,-409 190.5,-409 190.5,-440\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-416\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139915435522384&#45;&gt;139915434597840 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139915435522384&#45;&gt;139915434597840</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M163.5,-408.9604C163.5,-399.6356 163.5,-387.6748 163.5,-377.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"167.0001,-377.35 163.5,-367.3501 160.0001,-377.3501 167.0001,-377.35\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Pytorch, the computing graph is **dynamic**, and is constructed on the fly. You can make it as complex as you want it, for instance, adding control flow statements (e.g., if statements) to control the flow of the gradients."
      ],
      "metadata": {
        "id": "UOFeboryQNQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "0dBwxOqP9Ovn",
        "outputId": "0ad67e56-f934-478e-aa21-4f6951381c8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f4099c86c90>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"326pt\" height=\"503pt\"\n viewBox=\"0.00 0.00 326.00 503.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 499)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-499 322,-499 322,4 -4,4\"/>\n<!-- 139915434580688 -->\n<g id=\"node1\" class=\"node\">\n<title>139915434580688</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"242.5,-31 188.5,-31 188.5,0 242.5,0 242.5,-31\"/>\n<text text-anchor=\"middle\" x=\"215.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 139915435074768 -->\n<g id=\"node2\" class=\"node\">\n<title>139915435074768</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"260,-86 171,-86 171,-67 260,-67 260,-86\"/>\n<text text-anchor=\"middle\" x=\"215.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 139915435074768&#45;&gt;139915434580688 -->\n<g id=\"edge14\" class=\"edge\">\n<title>139915435074768&#45;&gt;139915434580688</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M215.5,-66.9688C215.5,-60.1289 215.5,-50.5621 215.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"219.0001,-41.3678 215.5,-31.3678 212.0001,-41.3678 219.0001,-41.3678\"/>\n</g>\n<!-- 139915435075152 -->\n<g id=\"node3\" class=\"node\">\n<title>139915435075152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"225,-141 130,-141 130,-122 225,-122 225,-141\"/>\n<text text-anchor=\"middle\" x=\"177.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 139915435075152&#45;&gt;139915435074768 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139915435075152&#45;&gt;139915435074768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M184.1191,-121.9197C189.2863,-114.4409 196.6174,-103.8301 202.8944,-94.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"205.8961,-96.5577 208.7009,-86.3408 200.137,-92.5786 205.8961,-96.5577\"/>\n</g>\n<!-- 139915434683344 -->\n<g id=\"node4\" class=\"node\">\n<title>139915434683344</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"205,-196 116,-196 116,-177 205,-177 205,-196\"/>\n<text text-anchor=\"middle\" x=\"160.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 139915434683344&#45;&gt;139915435075152 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139915434683344&#45;&gt;139915435075152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M163.4612,-176.9197C165.6524,-169.8304 168.7135,-159.9269 171.4182,-151.1763\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"174.8491,-151.9285 174.4583,-141.3408 168.1612,-149.8613 174.8491,-151.9285\"/>\n</g>\n<!-- 139915434683728 -->\n<g id=\"node5\" class=\"node\">\n<title>139915434683728</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"205,-251 116,-251 116,-232 205,-232 205,-251\"/>\n<text text-anchor=\"middle\" x=\"160.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 139915434683728&#45;&gt;139915434683344 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139915434683728&#45;&gt;139915434683344</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M160.5,-231.9197C160.5,-224.9083 160.5,-215.1442 160.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.0001,-206.3408 160.5,-196.3408 157.0001,-206.3409 164.0001,-206.3408\"/>\n</g>\n<!-- 139915434683920 -->\n<g id=\"node6\" class=\"node\">\n<title>139915434683920</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"205,-306 116,-306 116,-287 205,-287 205,-306\"/>\n<text text-anchor=\"middle\" x=\"160.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 139915434683920&#45;&gt;139915434683728 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139915434683920&#45;&gt;139915434683728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M160.5,-286.9197C160.5,-279.9083 160.5,-270.1442 160.5,-261.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"164.0001,-261.3408 160.5,-251.3408 157.0001,-261.3409 164.0001,-261.3408\"/>\n</g>\n<!-- 139915434684048 -->\n<g id=\"node7\" class=\"node\">\n<title>139915434684048</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-361 0,-361 0,-342 101,-342 101,-361\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139915434684048&#45;&gt;139915434683920 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139915434684048&#45;&gt;139915434683920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M69.6605,-341.9197C86.9053,-333.2973 112.476,-320.512 132.0978,-310.7011\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"133.8109,-313.7577 141.19,-306.155 130.6804,-307.4967 133.8109,-313.7577\"/>\n</g>\n<!-- 139915434601936 -->\n<g id=\"node8\" class=\"node\">\n<title>139915434601936</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-428 23.5,-428 23.5,-397 77.5,-397 77.5,-428\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139915434601936&#45;&gt;139915434684048 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139915434601936&#45;&gt;139915434684048</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-396.791C50.5,-389.0249 50.5,-379.5706 50.5,-371.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-371.0647 50.5,-361.0648 47.0001,-371.0648 54.0001,-371.0647\"/>\n</g>\n<!-- 139915434684176 -->\n<g id=\"node9\" class=\"node\">\n<title>139915434684176</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"208,-361 119,-361 119,-342 208,-342 208,-361\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 139915434684176&#45;&gt;139915434683920 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139915434684176&#45;&gt;139915434683920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M162.9774,-341.9197C162.595,-334.9083 162.0624,-325.1442 161.589,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"165.0763,-316.1353 161.0368,-306.3408 158.0867,-316.5166 165.0763,-316.1353\"/>\n</g>\n<!-- 139915434683664 -->\n<g id=\"node10\" class=\"node\">\n<title>139915434683664</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"267,-422 166,-422 166,-403 267,-403 267,-422\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-410\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 139915434683664&#45;&gt;139915434684176 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139915434683664&#45;&gt;139915434684176</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M208.2188,-402.9688C200.3307,-393.8901 188.2686,-380.0072 178.6588,-368.947\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"181.0988,-366.4188 171.898,-361.1656 175.8147,-371.0099 181.0988,-366.4188\"/>\n</g>\n<!-- 139915434683856 -->\n<g id=\"node14\" class=\"node\">\n<title>139915434683856</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"315,-361 226,-361 226,-342 315,-342 315,-361\"/>\n<text text-anchor=\"middle\" x=\"270.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 139915434683664&#45;&gt;139915434683856 -->\n<g id=\"edge13\" class=\"edge\">\n<title>139915434683664&#45;&gt;139915434683856</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M224.9375,-402.9688C232.9743,-393.8901 245.2641,-380.0072 255.0551,-368.947\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"257.9358,-370.9732 261.9435,-361.1656 252.6944,-366.3333 257.9358,-370.9732\"/>\n</g>\n<!-- 139915434599248 -->\n<g id=\"node11\" class=\"node\">\n<title>139915434599248</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"243.5,-495 189.5,-495 189.5,-464 243.5,-464 243.5,-495\"/>\n<text text-anchor=\"middle\" x=\"216.5\" y=\"-471\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139915434599248&#45;&gt;139915434683664 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139915434599248&#45;&gt;139915434683664</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M216.5,-463.9604C216.5,-454.6356 216.5,-442.6748 216.5,-432.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"220.0001,-432.35 216.5,-422.3501 213.0001,-432.3501 220.0001,-432.35\"/>\n</g>\n<!-- 139915435075344 -->\n<g id=\"node12\" class=\"node\">\n<title>139915435075344</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"318,-196 223,-196 223,-177 318,-177 318,-196\"/>\n<text text-anchor=\"middle\" x=\"270.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 139915435075344&#45;&gt;139915435074768 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139915435075344&#45;&gt;139915435074768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M265.6191,-176.7382C256.4636,-158.4271 236.5757,-118.6514 224.791,-95.082\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"227.8941,-93.4618 220.2914,-86.0828 221.6331,-96.5924 227.8941,-93.4618\"/>\n</g>\n<!-- 139915434683792 -->\n<g id=\"node13\" class=\"node\">\n<title>139915434683792</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"315,-306 226,-306 226,-287 315,-287 315,-306\"/>\n<text text-anchor=\"middle\" x=\"270.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 139915434683792&#45;&gt;139915435075344 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139915434683792&#45;&gt;139915435075344</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M270.5,-286.7382C270.5,-268.7541 270.5,-230.0652 270.5,-206.3599\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"274.0001,-206.0828 270.5,-196.0828 267.0001,-206.0829 274.0001,-206.0828\"/>\n</g>\n<!-- 139915434683856&#45;&gt;139915434683792 -->\n<g id=\"edge12\" class=\"edge\">\n<title>139915434683856&#45;&gt;139915434683792</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M270.5,-341.9197C270.5,-334.9083 270.5,-325.1442 270.5,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"274.0001,-316.3408 270.5,-306.3408 267.0001,-316.3409 274.0001,-316.3408\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "yhat = b + w * x_train_tensor\n",
        "error = yhat - y_train_tensor\n",
        "loss = (error ** 2).mean()\n",
        "\n",
        "# this makes no sense!!\n",
        "if loss > 0:\n",
        "    yhat2 = w * x_train_tensor\n",
        "    error2 = yhat2 - y_train_tensor\n",
        "\n",
        "# neither does this :-)\n",
        "loss += error2.mean()\n",
        "\n",
        "make_dot(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12y7f_bT9Ovn"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, weâve been manually updating the parameters using the computed gradients. Thatâs probably fine for two parameters, but what if we had a whole lot of them? We use one of **PyTorchâs optimizers**, like SGD or Adam.\n",
        "\n",
        "An optimizer takes the parameters we want to update, the learning rate we want to use (and many other hyper-parameters as well) and performs the updates through its `step()` method.\n",
        "\n",
        "Another advantage is that we don't need to zero the gradients one by one, we just invoke the optimizerâs `zero_grad()` method.\n"
      ],
      "metadata": {
        "id": "lMeypRAgTyNl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXeUkR3H9Ovn",
        "outputId": "ad671f39-5acd-4a74-a7c6-8688cd1c0cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 3.191796\n",
            "Epoch 100, Loss 0.013682\n",
            "Epoch 200, Loss 0.008313\n",
            "Epoch 300, Loss 0.008057\n",
            "Epoch 400, Loss 0.008045\n",
            "Epoch 500, Loss 0.008045\n",
            "Epoch 600, Loss 0.008045\n",
            "Epoch 700, Loss 0.008045\n",
            "Epoch 800, Loss 0.008045\n",
            "Epoch 900, Loss 0.008045\n",
            "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "lr = 0.1\n",
        "\n",
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "# DEFINES A SGD OPTIMIZER to update the parameters\n",
        "optimizer = optim.SGD([b, w], lr=lr)\n",
        "\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Step 1 - Computes model's predicted output - forward pass\n",
        "    yhat = b + w * x_train_tensor\n",
        "\n",
        "    # Step 2 - Computes the loss\n",
        "    error = (yhat - y_train_tensor)\n",
        "    loss = (error ** 2).mean()\n",
        "\n",
        "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Step 4 - Updates parameters using gradients and the learning rate. No more manual update!\n",
        "    # with torch.no_grad():\n",
        "    #     b -= lr * b.grad\n",
        "    #     w -= lr * w.grad\n",
        "    optimizer.step()\n",
        "\n",
        "    # No more telling Pytorch to let gradients go!\n",
        "    # b.grad.zero_()\n",
        "    # w.grad.zero_()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsJpvMYs9Ovo"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, there are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)  to choose from, depending on the task at hand. Since ours is a regression, we are using the Mean Square Error (MSE) loss."
      ],
      "metadata": {
        "id": "3FKXPn0WWxCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J34DuD4g9Ovo",
        "outputId": "b2213c13-7052-4d59-b331-646dc4a1d997"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MSELoss()"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "loss_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAwrhm3u9Ovo",
        "outputId": "57041525-6009-4366-8a5f-0b8e80675fb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1700)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "# This is a random example to illustrate the loss function\n",
        "predictions = torch.tensor([0.5, 1.0])\n",
        "labels = torch.tensor([2.0, 1.3])\n",
        "loss_fn(predictions, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXFfJAjR9Ovp",
        "outputId": "f55d5912-c67e-4c88-8492-1f5ada9d94b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 3.191796\n",
            "Epoch 100, Loss 0.013682\n",
            "Epoch 200, Loss 0.008313\n",
            "Epoch 300, Loss 0.008057\n",
            "Epoch 400, Loss 0.008045\n",
            "Epoch 500, Loss 0.008045\n",
            "Epoch 600, Loss 0.008045\n",
            "Epoch 700, Loss 0.008045\n",
            "Epoch 800, Loss 0.008045\n",
            "Epoch 900, Loss 0.008045\n",
            "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Sets learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "w = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "optimizer = optim.SGD([b, w], lr=lr)\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Step 1 - Computes model's predicted output - forward pass\n",
        "    yhat = b + w * x_train_tensor\n",
        "\n",
        "    # Step 2 - Computes the loss\n",
        "    # No more manual loss!\n",
        "    # error = (yhat - y_train_tensor)\n",
        "    # loss = (error ** 2).mean()\n",
        "    loss = loss_fn(yhat, y_train_tensor)\n",
        "\n",
        "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Step 4 - Updates parameters using gradients and\n",
        "    # the learning rate\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "print(b, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcBCH0K_9Ovp",
        "outputId": "5a032f96-fb06-450c-9e6a-e2ad0cbe1afe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "# Final loss value\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avuU4W2B9Ovp"
      },
      "outputs": [],
      "source": [
        "loss.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3YR-QtP9Ovp"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we know how to train a model, but, how we use it to make predictions of new data?"
      ],
      "metadata": {
        "id": "mKvvdRiGZ09M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, a model is represented by a regular Python class that inherits from the [Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class.\n",
        "\n",
        "The most fundamental methods it needs to implement are:\n",
        "- The constructor `__init__(self)`: it defines the parts that make up the model, in our case, two parameters, $a$ and $w$.\n",
        "- `forward(self, x)`: it performs the actual computation, that is, it outputs a prediction, given the input x.\n",
        "\n",
        "  __Note:__ You should NOT call the `forward(x)` method, though. You should call the whole model itself, as in `model(x)` to perform a forward pass and output predictions."
      ],
      "metadata": {
        "id": "BlVDKy1EaI64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSnUjCHM9Ovq"
      },
      "outputs": [],
      "source": [
        "class ManualLinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # To make \"b\" and \"w\" real parameters of the model, we need to wrap them with nn.Parameter class\n",
        "        # to tell PyTorch these tensors should be considered parameters of the model they are an attribute of\n",
        "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "        self.w = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Computes the outputs / predictions\n",
        "        return self.b + self.w * x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that, it is important to use the `Parameter` class with our parameters. By doing so, we can use our modelâs `parameters()` method to retrieve an iterator over all modelâs parameters, even those parameters of nested models, that we can use to feed our optimizer (instead of building a list of parameters ourselves!).\n",
        "\n",
        "Moreover, we can get the current values for all parameters using our modelâs `state_dict()` method."
      ],
      "metadata": {
        "id": "3UHnYdqLbyKg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M23MDUrO9Ovq"
      },
      "source": [
        "## state_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the learnable parameters (i.e. weights and biases) of a `torch.nn.Module` model are contained in the modelâs parameters (accessed with `model.parameters()`). A **`state_dict`** is simply a Python dictionary object that maps each layer to its parameter tensor.\n",
        "\n",
        "A `state_dict` is an integral entity if you are interested in saving or loading models from PyTorch. Because `state_dict` objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers.\n",
        "\n",
        "Note that **only layers with learnable parameters** (convolutional layers, linear layers, etc.) and registered buffers (batchnormâs running_mean) have entries in the modelâs state_dict. **Optimizer objects** (torch.optim) also have a state_dict, which contains information about the optimizerâs state, as well as the hyperparameters used."
      ],
      "metadata": {
        "id": "L8G74HZNy8zn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq3vkBxv9Ovq",
        "outputId": "d6cdac75-d9e5-46ba-accb-f4eb75126270"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True), Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
        "dummy = ManualLinearRegression()\n",
        "list(dummy.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reiDCyJ99Ovq",
        "outputId": "1dc3ff26-00b4-4c81-c7be-ffd9d3e1aff3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('b', tensor([0.3367])), ('w', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "dummy.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKTyC_Uh9Ovq",
        "outputId": "dc166adc-cfc6-401c-cdfb-7a15f9276be4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'state': {0: {'momentum_buffer': None}, 1: {'momentum_buffer': None}},\n",
              " 'param_groups': [{'lr': 0.1,\n",
              "   'momentum': 0,\n",
              "   'dampening': 0,\n",
              "   'weight_decay': 0,\n",
              "   'nesterov': False,\n",
              "   'maximize': False,\n",
              "   'foreach': None,\n",
              "   'params': [0, 1]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "optimizer.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDZS4Wdd9Ovq"
      },
      "source": [
        "## device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to send our model to the same device where the data is. If our data is made of GPU tensors, our model must \"*live*\" inside the GPU as well."
      ],
      "metadata": {
        "id": "jDoBSVxpuQtj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad3jyPTe9Ovr"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "# Creates a \"dummy\" instance of our ManualLinearRegression model\n",
        "# and sends it to the device\n",
        "dummy = ManualLinearRegression().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzPFsgco9Ovr"
      },
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVk8r0Df9Ovr",
        "outputId": "ad9cb87a-0a1c-418b-97b7-2e434139ec28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 2.747487\n",
            "Epoch 100, Loss 0.014369\n",
            "Epoch 200, Loss 0.008346\n",
            "Epoch 300, Loss 0.008059\n",
            "Epoch 400, Loss 0.008045\n",
            "Epoch 500, Loss 0.008045\n",
            "Epoch 600, Loss 0.008045\n",
            "Epoch 700, Loss 0.008045\n",
            "Epoch 800, Loss 0.008045\n",
            "Epoch 900, Loss 0.008045\n",
            "OrderedDict([('b', tensor([1.0235], device='cuda:0')), ('w', tensor([1.9690], device='cuda:0'))])\n"
          ]
        }
      ],
      "source": [
        "# Sets learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "# Now we can create a model and send it at once to the device\n",
        "model = ManualLinearRegression().to(device)\n",
        "\n",
        "# Defines a SGD optimizer to update the parameters\n",
        "# (now retrieved directly from the model)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train() # What is this???\n",
        "\n",
        "    # Step 1 - Computes model's predicted output - forward pass\n",
        "    # No more manual prediction!\n",
        "    yhat = model(x_train_tensor)\n",
        "\n",
        "    # Step 2 - Computes the loss\n",
        "    loss = loss_fn(yhat, y_train_tensor)\n",
        "\n",
        "    # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Step 4 - Updates parameters using gradients and\n",
        "    # the learning rate\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "# We can also inspect its parameters using its state_dict\n",
        "print(model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-thOmfZF9Ovr"
      },
      "source": [
        "## Train\n",
        "\n",
        "In PyTorch, models have a `train()` method which, somewhat disappointingly, does NOT perform a training step. Its only purpose is to set the model to training mode. Why is this important? Some models may use mechanisms like Dropout, for instance, which have distinct behaviors in training and evaluation phases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eKh4RPH9Ovr"
      },
      "outputs": [],
      "source": [
        "## Never forget to include model.train() in your training loop!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTm69bUD9Ovs"
      },
      "source": [
        "## Nested Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our model class, we can create very complex models, such as encoder-decoder with specific neural network architectures (convolutional, recurrent, etc). Each one of these architectures are models itself, *nested* in our model class.\n",
        "\n",
        "Let's see a simple example, where will use PyTorchâs [linear model](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) as an attribute of our own, thus creating a nested model."
      ],
      "metadata": {
        "id": "jm1Lv1uGeXuG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kVL7r9c9Ovs",
        "outputId": "038ab5e8-61d9-46f5-f605-472e68a8d79d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=1, out_features=1, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "linear = nn.Linear(1, 1)\n",
        "linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG36Z86K9Ovs",
        "outputId": "6657d5c9-de7f-4d57-f7ea-2837d0ffe838"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight', tensor([[-0.2191]])), ('bias', tensor([0.2018]))])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ],
      "source": [
        "linear.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `__init__` method, we created an attribute that contains our nested Linear model.\n",
        "\n",
        "In the `forward()` method, we call the nested model itself to perform the forward pass (notice, we are not calling `self.linear.forward(x)`!)."
      ],
      "metadata": {
        "id": "cEzg9bIezh5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qSmQlnZ9Ovs"
      },
      "outputs": [],
      "source": [
        "class LayerLinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Instead of our custom parameters, we use a Linear model with single input and single output\n",
        "        self.linear = nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Now it only takes a call to the layer to make predictions\n",
        "        self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we call the `parameters()` method of this model, PyTorch will figure the parameters of its attributes in a recursive way. You can try it yourself using something like: `[*LayerLinearRegression().parameters()]` to get a list of all parameters. You can also add new Linear attributes and, even if you donât use them at all in the forward pass, they will still be listed under `parameters()`."
      ],
      "metadata": {
        "id": "1vVRDpBt4D2c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uEBkac_9Ovs",
        "outputId": "5fef77a3-315b-4be7-dd4e-9184cc0bb608"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[0.7645]], device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([0.8300], device='cuda:0', requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "dummy = LayerLinearRegression().to(device)\n",
        "list(dummy.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q11y7RhH9Ovs",
        "outputId": "15c4c588-fe37-4daf-ccd6-74772753a8af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight', tensor([[0.7645]], device='cuda:0')),\n",
              "             ('linear.bias', tensor([0.8300], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "dummy.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dleA8nqV9Ovt"
      },
      "source": [
        "## Sequential Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep neural networks, included those we will use in this course, are *sequential* models composed of layers, where the output of a layer is sequentially fed as an input to the next.\n",
        "\n",
        "In our simple lineal model example, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:"
      ],
      "metadata": {
        "id": "1uvcT3wVEQb-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KdYvy6F9Ovt",
        "outputId": "21ccb6e0-b692-4f3d-ad96-5317cdc39e2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[0.7645]], device='cuda:0')),\n",
              "             ('0.bias', tensor([0.8300], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# Alternatively, you can use a Sequential model\n",
        "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR1dAw8W9Ovt"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In nested sequential models, any internal model is a **layer**.\n",
        "\n",
        "A Linear model can be seen as a layer in a neural network:\n",
        "\n",
        "![landscape](https://drive.google.com/uc?id=1V3Tg9uKxVfXmpXp9s86JWV2raoucbQML)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QD-IdwTfNdC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgwpLE0p9Ovt",
        "outputId": "f573ee8e-d9b0-44f7-9343-4fef12b95c77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
              "                      [ 0.5304, -0.1265,  0.1165],\n",
              "                      [-0.2811,  0.3391,  0.5090],\n",
              "                      [-0.4236,  0.5018,  0.1081],\n",
              "                      [ 0.4266,  0.0782,  0.2784]], device='cuda:0')),\n",
              "             ('0.bias',\n",
              "              tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')),\n",
              "             ('1.weight',\n",
              "              tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')),\n",
              "             ('1.bias', tensor([-0.2062], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# Building the model from the figure above\n",
        "model = nn.Sequential(nn.Linear(3, 5), nn.Linear(5, 1)).to(device)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this sequential model does not have attribute names, state_dict() uses numeric prefixes.\n",
        "\n",
        "You can also use a modelâs `add_module()` method to be able to name the layers:"
      ],
      "metadata": {
        "id": "NTpktEq-UYiv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B5DCTCy9Ovt",
        "outputId": "2b3f4313-01ac-42eb-f93b-69dba5fcc597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer1.weight', tensor([[ 0.4414,  0.4792, -0.1353],\n",
              "                      [ 0.5304, -0.1265,  0.1165],\n",
              "                      [-0.2811,  0.3391,  0.5090],\n",
              "                      [-0.4236,  0.5018,  0.1081],\n",
              "                      [ 0.4266,  0.0782,  0.2784]], device='cuda:0')),\n",
              "             ('layer1.bias',\n",
              "              tensor([-0.0815,  0.4451,  0.0853, -0.2695,  0.1472], device='cuda:0')),\n",
              "             ('layer2.weight',\n",
              "              tensor([[-0.2060, -0.0524, -0.1816,  0.2967, -0.3530]], device='cuda:0')),\n",
              "             ('layer2.bias', tensor([-0.2062], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# Building the model from the figure above\n",
        "model = nn.Sequential()\n",
        "model.add_module('layer1', nn.Linear(3, 5))\n",
        "model.add_module('layer2', nn.Linear(5, 1))\n",
        "model.to(device)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are **many** different [layers](https://pytorch.org/docs/stable/nn.html) that can be used in PyTorch. We will see many of them in this course."
      ],
      "metadata": {
        "id": "wp7W93_IU9bW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPS6WC3_9Ovt"
      },
      "source": [
        "# Putting It All Together"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model configuration.\n",
        "\n",
        "Here, we will include these elements:\n",
        "\n",
        "- a model\n",
        "- a loss function (which needs to be chosen according to your model)\n",
        "- an optimizer"
      ],
      "metadata": {
        "id": "UaAud7VN2WI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Sets learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Step 0 - Initializes parameters \"b\" and \"w\" randomly\n",
        "torch.manual_seed(42)\n",
        "# Now we can create A MODEL and send it at once to the device\n",
        "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
        "\n",
        "# Defines a SGD OPTIMIZER to update the parameters\n",
        "# (now retrieved directly from the model)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Defines a MSE LOSS FUNCTION\n",
        "loss_fn = nn.MSELoss(reduction='mean')"
      ],
      "metadata": {
        "id": "fXGecC_53Hn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2WoaSV9Ovu"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Once defined the model, and given a set of **features** and **labels**, the training loops over the gradient descent steps weâve seen at the beginning of this chapter:\n",
        "\n",
        "1.   compute modelâs predictions\n",
        "2.   compute the loss\n",
        "3.   compute the gradients\n",
        "4.   update the parameters\n",
        "\n",
        "This sequence is repeated over and over until the number of epochs is reached."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    # Builds function that performs a step in the train loop\n",
        "    def train_step(x, y):\n",
        "        # Sets model to TRAIN mode\n",
        "        model.train()\n",
        "        # Makes predictions\n",
        "        yhat = model(x)\n",
        "        # Computes loss\n",
        "        loss = loss_fn(y, yhat)\n",
        "        # Computes gradients\n",
        "        loss.backward()\n",
        "        # Updates parameters and zeroes gradients\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Returns the loss\n",
        "        return loss.item()\n",
        "\n",
        "    # Returns the function that will be called inside the train loop\n",
        "    return train_step\n",
        "\n",
        "# Creates the train_step function for our model, loss function and optimizer\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "losses = []\n",
        "\n",
        "# Defines number of epochs\n",
        "n_epochs = 1000\n",
        "# For each epoch...\n",
        "for epoch in range(n_epochs):\n",
        "    # Performs one train step and returns the corresponding loss\n",
        "    loss = train_step(x_train_tensor, y_train_tensor)\n",
        "    losses.append(loss)\n",
        "\n",
        "# Checks model's parameters\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PTUAo4246mz",
        "outputId": "0fdbc678-0d61-42f2-ef29-b7f319b974aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[1.9690]], device='cuda:0')), ('0.bias', tensor([1.0235], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses"
      ],
      "metadata": {
        "id": "z9w5CxcEdFJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Would `make_train_step` code change if we were using a different optimizer, or loss, or even model?"
      ],
      "metadata": {
        "id": "ANzBcUF668SC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "So far, the only data preparation we performed was transforming Numpy arrays into PyTorch tensors, but in Pytorch, we have a useful class and methods to handle it.\n",
        "\n",
        "In PyTorch, a dataset is represented by a regular Python class that inherits from the **Dataset class**. You can think of it as a kind of a Python list of tuples, each tuple corresponding to one point (features, label).\n",
        "\n",
        "The The most fundamental methods it needs to implement are:\n",
        "\n",
        "- `__init__(self)` : it takes whatever arguments needed to build a list of tuples â it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n",
        "\n",
        "  IMPORTANT: There is no need to load the whole dataset in the constructor method (`__init__`). If your dataset is big (tens of thousands of image files, for instance), loading it at once would not be memory efficient. It is recommended to load them on demand (whenever `__get_item__` is called). We will see more of this soon.\n",
        "\n",
        "- `__get_item__(self, index)`: it allows the dataset to be indexed, so it can work like a list (`dataset[i]`) â it must return a tuple (features, label) corresponding to the requested data point. We can either return the corresponding slices of our pre-loaded dataset or tensors or, as mentioned above, load them on demand.\n",
        "\n",
        "- `__len__(self)`: it should simply return the size of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size"
      ],
      "metadata": {
        "id": "MMVTUSjU7Y9Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuVovFs89Ovv",
        "outputId": "830fa839-8d69-4f57-ebd2-a06038f286b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n",
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_tensor, y_tensor):\n",
        "        self.x = x_tensor\n",
        "        self.y = y_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.x[index], self.y[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])\n",
        "\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVE** that we built our training tensors out of Numpy arrays but we did not send them to a device? So, they are CPU tensors now!\n",
        "\n",
        "We donât want our whole training data to be loaded into GPU tensors, as we have been doing in our example so far, because it takes up space in our precious graphics cardâs RAM."
      ],
      "metadata": {
        "id": "Z0nB5CSY9wjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "In our simple linear regression example, we used batch gradient descent, (i.e., we used the whole training data at every training step), but generally, we will use mini-batch gradient descent. Thus, we need mini-batches (\"*slices*\" of our dataset)\n",
        "\n",
        "We can use use PyTorchâs **DataLoader** class. We tell it which dataset to use, the desired mini-batch size and if weâd like to shuffle it or not.\n",
        "\n",
        "Our loader will behave like an iterator, so we can loop over it and fetch a different mini-batch every time."
      ],
      "metadata": {
        "id": "d5mghbDb-ZkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
        "\n",
        "losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # the dataset \"lives\" in the CPU, so do our mini-batches therefore, we need to send those mini-batches to the\n",
        "        # device where the model \"lives\"\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sATbN1PmBiI0",
        "outputId": "058f4510-e28f-4a87-b8c1-dd6e865b56b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss 0.010778\n",
            "Epoch 0, Loss 0.009254\n",
            "Epoch 0, Loss 0.005545\n",
            "Epoch 0, Loss 0.009093\n",
            "Epoch 0, Loss 0.005926\n",
            "Epoch 100, Loss 0.005049\n",
            "Epoch 100, Loss 0.008517\n",
            "Epoch 100, Loss 0.012365\n",
            "Epoch 100, Loss 0.010788\n",
            "Epoch 100, Loss 0.004192\n",
            "Epoch 200, Loss 0.009954\n",
            "Epoch 200, Loss 0.010472\n",
            "Epoch 200, Loss 0.007589\n",
            "Epoch 200, Loss 0.006612\n",
            "Epoch 200, Loss 0.005935\n",
            "Epoch 300, Loss 0.010226\n",
            "Epoch 300, Loss 0.010650\n",
            "Epoch 300, Loss 0.007348\n",
            "Epoch 300, Loss 0.006183\n",
            "Epoch 300, Loss 0.006084\n",
            "Epoch 400, Loss 0.008207\n",
            "Epoch 400, Loss 0.005627\n",
            "Epoch 400, Loss 0.013359\n",
            "Epoch 400, Loss 0.006138\n",
            "Epoch 400, Loss 0.007319\n",
            "Epoch 500, Loss 0.007649\n",
            "Epoch 500, Loss 0.010010\n",
            "Epoch 500, Loss 0.008143\n",
            "Epoch 500, Loss 0.007255\n",
            "Epoch 500, Loss 0.007613\n",
            "Epoch 600, Loss 0.007066\n",
            "Epoch 600, Loss 0.003961\n",
            "Epoch 600, Loss 0.008639\n",
            "Epoch 600, Loss 0.011508\n",
            "Epoch 600, Loss 0.009555\n",
            "Epoch 700, Loss 0.010578\n",
            "Epoch 700, Loss 0.004490\n",
            "Epoch 700, Loss 0.010441\n",
            "Epoch 700, Loss 0.007924\n",
            "Epoch 700, Loss 0.006942\n",
            "Epoch 800, Loss 0.008312\n",
            "Epoch 800, Loss 0.008683\n",
            "Epoch 800, Loss 0.006224\n",
            "Epoch 800, Loss 0.007478\n",
            "Epoch 800, Loss 0.009903\n",
            "Epoch 900, Loss 0.010934\n",
            "Epoch 900, Loss 0.006465\n",
            "Epoch 900, Loss 0.009129\n",
            "Epoch 900, Loss 0.005205\n",
            "Epoch 900, Loss 0.009051\n",
            "OrderedDict([('0.weight', tensor([[1.9696]], device='cuda:0')), ('0.bias', tensor([1.0243], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe that, now, we have an inner loop to load each and every mini-batch from our DataLoader and, more importantly, we are now sending only one mini-batch to the device.\n",
        "\n",
        "For bigger datasets, loading data sample by sample (into a CPU tensor) using Datasetâs `__get_item__` and then sending all samples that belong to the same mini-batch at once to your GPU (device) is the way to go in order to make the best use of your graphics cardâs RAM. Moreover, if you have many GPUs to train your model on, it is best to keep your dataset âagnosticâ and assign the batches to different GPUs during training."
      ],
      "metadata": {
        "id": "3d9i93YlCYly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "So far, weâve focused on the training data only. We built a dataset and a data loader for it. We could do the same for the validation data.\n",
        "\n",
        "PyTorchâs `random_split()` method is an easy and familiar way of performing a training-validation split. Just keep in mind that, in our example, we need to apply it to the whole dataset. Then, for each subset of data, we build a corresponding **DataLoader**.\n",
        "\n",
        "Once we have a data loader for our validation set, so, it makes sense to use it for the **Evaluation** step:"
      ],
      "metadata": {
        "id": "3qNxXxEnDAeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "\n",
        "# defines train and validation datasets\n",
        "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
        "\n",
        "# now, we create a DataLoader for each dataset\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "# training and validation steps\n",
        "losses = []\n",
        "val_losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            yhat = model(x_val)\n",
        "            val_loss = loss_fn(y_val, yhat)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LFCdwLWDxS4",
        "outputId": "d91214cb-a4e8-48a4-ac27-e7dce330fb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[1.9625]], device='cuda:0')), ('0.bias', tensor([1.0196], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two observations respecting to Evaluation:\n",
        "\n",
        "- `torch.no_grad()`: even though it wonât make a difference in our simple model, it is a good practice to wrap the validation inner loop with this context manager to disable any gradient calculation that you may inadvertently trigger (remember: *gradients belong in training, not in validation steps*)\n",
        "\n",
        "- `eval()`: the only thing it does is setting the model to evaluation mode (just like its `train()` counterpart did), so the model can adjust its behavior regarding some operations, like Dropout."
      ],
      "metadata": {
        "id": "DCpBlskFJRPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_losses"
      ],
      "metadata": {
        "id": "lWQon30CE0Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final model"
      ],
      "metadata": {
        "id": "Cg_6DLyTIwMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "# Builds dataset with ALL data\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "# Splits randomly into train and validation datasets\n",
        "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
        "# Builds a loader for each dataset to perform mini-batch gradient descent\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
        "\n",
        "# Builds a simple sequential model\n",
        "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
        "print(model.state_dict())\n",
        "\n",
        "# Sets hyper-parameters\n",
        "lr = 1e-1\n",
        "n_epochs = 100\n",
        "\n",
        "# Defines loss function and optimizer\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "losses = []\n",
        "val_losses = []\n",
        "# Creates function to perform train step from model, loss and optimizer\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    # Uses loader to fetch one mini-batch for training\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        # NOW, sends the mini-batch data to the device\n",
        "        # so it matches location of the MODEL\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        # One stpe of training\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "\n",
        "    # After finishing training steps for all mini-batches,\n",
        "    # it is time for evaluation!\n",
        "\n",
        "    # We tell PyTorch to NOT use autograd...\n",
        "    # Do you remember why?\n",
        "    with torch.no_grad():\n",
        "        # Uses loader to fetch one mini-batch for validation\n",
        "        for x_val, y_val in val_loader:\n",
        "            # Again, sends data to same device as model\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "\n",
        "            # What is that?!\n",
        "            model.eval()\n",
        "            # Makes predictions\n",
        "            yhat = model(x_val)\n",
        "            # Computes validation loss\n",
        "            val_loss = loss_fn(y_val, yhat)\n",
        "            val_losses.append(val_loss.item())\n",
        "    print(f\"[{epoch+1}] Training loss: {loss:.3f}\\t Validation loss: {val_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYFjd7zBFrNv",
        "outputId": "70de5043-19ea-43df-e5dd-72535e6cfad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[-0.9676]], device='cuda:0')), ('0.bias', tensor([-0.5727], device='cuda:0'))])\n",
            "[1] Training loss: 1.051\t Validation loss: 0.849\n",
            "[2] Training loss: 0.227\t Validation loss: 0.210\n",
            "[3] Training loss: 0.170\t Validation loss: 0.139\n",
            "[4] Training loss: 0.150\t Validation loss: 0.115\n",
            "[5] Training loss: 0.131\t Validation loss: 0.098\n",
            "[6] Training loss: 0.114\t Validation loss: 0.085\n",
            "[7] Training loss: 0.100\t Validation loss: 0.074\n",
            "[8] Training loss: 0.088\t Validation loss: 0.064\n",
            "[9] Training loss: 0.077\t Validation loss: 0.056\n",
            "[10] Training loss: 0.068\t Validation loss: 0.049\n",
            "[11] Training loss: 0.060\t Validation loss: 0.043\n",
            "[12] Training loss: 0.053\t Validation loss: 0.037\n",
            "[13] Training loss: 0.047\t Validation loss: 0.033\n",
            "[14] Training loss: 0.042\t Validation loss: 0.029\n",
            "[15] Training loss: 0.037\t Validation loss: 0.026\n",
            "[16] Training loss: 0.034\t Validation loss: 0.023\n",
            "[17] Training loss: 0.030\t Validation loss: 0.021\n",
            "[18] Training loss: 0.028\t Validation loss: 0.019\n",
            "[19] Training loss: 0.025\t Validation loss: 0.017\n",
            "[20] Training loss: 0.023\t Validation loss: 0.016\n",
            "[21] Training loss: 0.021\t Validation loss: 0.015\n",
            "[22] Training loss: 0.020\t Validation loss: 0.014\n",
            "[23] Training loss: 0.019\t Validation loss: 0.013\n",
            "[24] Training loss: 0.017\t Validation loss: 0.012\n",
            "[25] Training loss: 0.017\t Validation loss: 0.011\n",
            "[26] Training loss: 0.016\t Validation loss: 0.011\n",
            "[27] Training loss: 0.015\t Validation loss: 0.010\n",
            "[28] Training loss: 0.014\t Validation loss: 0.010\n",
            "[29] Training loss: 0.014\t Validation loss: 0.010\n",
            "[30] Training loss: 0.013\t Validation loss: 0.009\n",
            "[31] Training loss: 0.013\t Validation loss: 0.009\n",
            "[32] Training loss: 0.013\t Validation loss: 0.009\n",
            "[33] Training loss: 0.012\t Validation loss: 0.009\n",
            "[34] Training loss: 0.012\t Validation loss: 0.009\n",
            "[35] Training loss: 0.012\t Validation loss: 0.008\n",
            "[36] Training loss: 0.012\t Validation loss: 0.008\n",
            "[37] Training loss: 0.011\t Validation loss: 0.008\n",
            "[38] Training loss: 0.011\t Validation loss: 0.008\n",
            "[39] Training loss: 0.011\t Validation loss: 0.008\n",
            "[40] Training loss: 0.011\t Validation loss: 0.008\n",
            "[41] Training loss: 0.011\t Validation loss: 0.008\n",
            "[42] Training loss: 0.011\t Validation loss: 0.008\n",
            "[43] Training loss: 0.011\t Validation loss: 0.008\n",
            "[44] Training loss: 0.011\t Validation loss: 0.008\n",
            "[45] Training loss: 0.011\t Validation loss: 0.008\n",
            "[46] Training loss: 0.011\t Validation loss: 0.008\n",
            "[47] Training loss: 0.011\t Validation loss: 0.008\n",
            "[48] Training loss: 0.011\t Validation loss: 0.008\n",
            "[49] Training loss: 0.011\t Validation loss: 0.008\n",
            "[50] Training loss: 0.011\t Validation loss: 0.008\n",
            "[51] Training loss: 0.011\t Validation loss: 0.008\n",
            "[52] Training loss: 0.011\t Validation loss: 0.008\n",
            "[53] Training loss: 0.011\t Validation loss: 0.008\n",
            "[54] Training loss: 0.011\t Validation loss: 0.008\n",
            "[55] Training loss: 0.010\t Validation loss: 0.008\n",
            "[56] Training loss: 0.010\t Validation loss: 0.008\n",
            "[57] Training loss: 0.010\t Validation loss: 0.008\n",
            "[58] Training loss: 0.010\t Validation loss: 0.008\n",
            "[59] Training loss: 0.010\t Validation loss: 0.008\n",
            "[60] Training loss: 0.010\t Validation loss: 0.008\n",
            "[61] Training loss: 0.010\t Validation loss: 0.008\n",
            "[62] Training loss: 0.010\t Validation loss: 0.008\n",
            "[63] Training loss: 0.010\t Validation loss: 0.008\n",
            "[64] Training loss: 0.010\t Validation loss: 0.008\n",
            "[65] Training loss: 0.010\t Validation loss: 0.008\n",
            "[66] Training loss: 0.010\t Validation loss: 0.008\n",
            "[67] Training loss: 0.010\t Validation loss: 0.008\n",
            "[68] Training loss: 0.010\t Validation loss: 0.008\n",
            "[69] Training loss: 0.010\t Validation loss: 0.008\n",
            "[70] Training loss: 0.010\t Validation loss: 0.008\n",
            "[71] Training loss: 0.010\t Validation loss: 0.008\n",
            "[72] Training loss: 0.010\t Validation loss: 0.008\n",
            "[73] Training loss: 0.010\t Validation loss: 0.008\n",
            "[74] Training loss: 0.010\t Validation loss: 0.008\n",
            "[75] Training loss: 0.010\t Validation loss: 0.008\n",
            "[76] Training loss: 0.010\t Validation loss: 0.008\n",
            "[77] Training loss: 0.010\t Validation loss: 0.008\n",
            "[78] Training loss: 0.010\t Validation loss: 0.008\n",
            "[79] Training loss: 0.010\t Validation loss: 0.008\n",
            "[80] Training loss: 0.010\t Validation loss: 0.008\n",
            "[81] Training loss: 0.010\t Validation loss: 0.008\n",
            "[82] Training loss: 0.010\t Validation loss: 0.008\n",
            "[83] Training loss: 0.010\t Validation loss: 0.008\n",
            "[84] Training loss: 0.010\t Validation loss: 0.008\n",
            "[85] Training loss: 0.010\t Validation loss: 0.008\n",
            "[86] Training loss: 0.010\t Validation loss: 0.008\n",
            "[87] Training loss: 0.010\t Validation loss: 0.008\n",
            "[88] Training loss: 0.010\t Validation loss: 0.008\n",
            "[89] Training loss: 0.010\t Validation loss: 0.008\n",
            "[90] Training loss: 0.010\t Validation loss: 0.008\n",
            "[91] Training loss: 0.010\t Validation loss: 0.008\n",
            "[92] Training loss: 0.010\t Validation loss: 0.008\n",
            "[93] Training loss: 0.010\t Validation loss: 0.008\n",
            "[94] Training loss: 0.010\t Validation loss: 0.008\n",
            "[95] Training loss: 0.010\t Validation loss: 0.008\n",
            "[96] Training loss: 0.010\t Validation loss: 0.008\n",
            "[97] Training loss: 0.010\t Validation loss: 0.008\n",
            "[98] Training loss: 0.010\t Validation loss: 0.008\n",
            "[99] Training loss: 0.010\t Validation loss: 0.008\n",
            "[100] Training loss: 0.010\t Validation loss: 0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict())\n",
        "print('mean training loss:', np.mean(losses))\n",
        "print('mean validation loss:', np.mean(val_losses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GTlDg_uGJlR",
        "outputId": "313e6d88-31dc-40fb-97de-2ba0e1893fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[1.9616]], device='cuda:0')), ('0.bias', tensor([1.0152], device='cuda:0'))])\n",
            "mean training loss: 0.0690538402562961\n",
            "mean validation loss: 0.0269468957465142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()['0.weight']\n",
        "model.state_dict()['0.bias']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXK5mb_4GhdO",
        "outputId": "98d467fa-83e0-4741-d568-1290e0aeb123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0152], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_minimum, w_minimum = model.state_dict()['0.bias'].cpu().detach().numpy()[0], model.state_dict()['0.weight'].cpu().detach().numpy()[0]\n",
        "# Generates evenly spaced x feature\n",
        "x_range = np.linspace(0, 1, 101)\n",
        "# Computes yhat\n",
        "yhat_range = b_minimum + w_minimum * x_range\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_ylim([0, 3.1])\n",
        "\n",
        "# Dataset\n",
        "ax.scatter(x_train, y_train)\n",
        "# Predictions\n",
        "ax.plot(x_range, yhat_range, label='Final model\\'s predictions', c='k', linestyle='--')\n",
        "\n",
        "# Annotations\n",
        "ax.legend(loc=0)\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "VBWcIHoLHhGJ",
        "outputId": "a5392d03-9c13-4320-da3d-9703e8c5975b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}